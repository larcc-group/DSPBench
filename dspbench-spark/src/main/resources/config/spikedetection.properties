sd.source.threads=1
sd.source.class=spark.streaming.source.KafkaSourceSS
sd.source.path=data/spikedetection
sd.source.parser=org.dspbench.applications.spikedetection.SensorParser

sd.sink.threads=1
sd.sink.class=spark.streaming.sink.ConsoleSinkSS

sd.parser.value_field=temp
sd.moving_average.threads=1
sd.moving_average.window=500
sd.spike_detector.threads=1
sd.spike_detector.threshold=0.03

metrics.enabled=true
metrics.reporter=csv
metrics.interval.unit=seconds
metrics.interval=1

kafka.maxRatePerPartition = 1000

sd.kafka.zookeeper.host=localhost:9092
sd.kafka.source.topic=spikeDetect2
sd.parser.threads=1
sd.checkpoint.path='/tmp'

sd.batch.size=1000
spark.streaming.backpressure.enabled=true
spark.streaming.backpressure.initialRate=1000
spark.streaming.receiver.maxRate=1000
spark.streaming.kafka.maxRatePerPartition=1000

metrics.output=/home/luan/Documents/trash/metrics
